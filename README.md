This project compares different sampling techniques when used up on various ML models for classification: following sampling techniques were used:

Random Sampling

Structured Sampling
Stratified sampling
cluster sampling
Weighted sampling
for the following ML models

logistic regression
Random forest classifier
Naive bayes
SVM
Decision Tree classifier
You can just run the file Main.py with the given dataset to find out which sampling is best for the following models PS: Oversampling using SMOTE was applied prior to applying these techniques.

Results
The results of the project showed that the best sampling technique for a particular machine 
learning model depends on the characteristics of the data set. For example, random sampling 
was found to be the best sampling technique for the logistic regression model, while 
stratified sampling was found to be the best sampling technique for the random forest 
classifier.
In general, it was found that stratified sampling and cluster sampling were the most effective 
sampling techniques for the machine learning models in the project. This is because these 
techniques ensure that the sample is representative of the population, which can improve the 
accuracy of the machine learning models.
