{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu-q1De16mvj"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Read the data\n",
        "data = pd.read_csv('Creditcard_data.csv')\n",
        "x=data.iloc[:,:-1]\n",
        "y=data.iloc[:,-1]\n",
        "print('previous shape',x.shape,y.shape)\n",
        "#use smote to oversample the data\n",
        "sm = SMOTE(random_state=42)\n",
        "x_res, y_res = sm.fit_resample(x, y)\n",
        "print(x_res.shape,y_res.shape)\n",
        "print('counter of 0',y_res.value_counts()[0])\n",
        "print('counter of 1',y_res.value_counts()[1])\n",
        "\n",
        "#apply random sampling the data to get 70% of the data\n",
        "#use test train split to split 80% of the data to train and 20% to test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "logistic=[]\n",
        "random=[]\n",
        "svm_li=[]\n",
        "naive_bayes_li=[]\n",
        "decision_tree_li=[]\n",
        "\n",
        "dict={0:'random sampling',1:'structured sampling',2:'stratified sampling',3:'cluster sampling'}\n",
        "\n",
        "oversampled_data=pd.DataFrame(x_res)\n",
        "oversampled_data['Class']=y_res\n",
        "\n",
        "print(oversampled_data.shape)\n",
        "\n",
        "train_set,test_set=train_test_split(oversampled_data,test_size=0.2,random_state=42)#now use train_set to train the model and dont use test_set\n",
        "\n",
        "#implementing random sampling here\n",
        "n=round(train_set.shape[0]*0.9)\n",
        "random_sample_train=train_set.sample(n=n,random_state=42)\n",
        "\n",
        "\n",
        "original_y=test_set.iloc[:,-1]#y labels of the test set\n",
        "test_set=test_set.iloc[:,:30]#x labels of the test set\n",
        "print('some stuff',random_sample_train.shape,test_set.shape,train_set.shape)\n",
        "\n",
        "#training logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def logistic_regression(random_sample_train):\n",
        "    clf=LogisticRegression(max_iter=10000)\n",
        "    clf.fit(random_sample_train.iloc[:,:30],random_sample_train.iloc[:,-1])\n",
        "    y_pred=clf.predict(test_set)\n",
        "    print('accuracy score for logistic  ', accuracy_score(original_y,y_pred))\n",
        "    return accuracy_score(original_y, y_pred)\n",
        "\n",
        "\n",
        "#training random forest model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def random_forest(random_sample_train):\n",
        "    clf=RandomForestClassifier()\n",
        "    clf.fit(random_sample_train.iloc[:,:30],random_sample_train.iloc[:,-1])\n",
        "    y_pred=clf.predict(test_set)\n",
        "    print('accuracy score for random forest' ,accuracy_score(original_y,y_pred))\n",
        "    return accuracy_score(original_y, y_pred)\n",
        "#training svm model\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def svm(random_sample_train):\n",
        "    clf=SVC()\n",
        "    clf.fit(random_sample_train.iloc[:,:30],random_sample_train.iloc[:,-1])\n",
        "    y_pred=clf.predict(test_set)\n",
        "    print('accuracy score for svm  ', accuracy_score(original_y,y_pred))\n",
        "    return accuracy_score(original_y, y_pred)\n",
        "#training naive bayes model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "def naive_bayes(random_sample_train):\n",
        "    clf=GaussianNB()\n",
        "    clf.fit(random_sample_train.iloc[:,:30],random_sample_train.iloc[:,-1])\n",
        "    y_pred=clf.predict(test_set)\n",
        "    print('accuracy score for naive bayes ', accuracy_score(original_y,y_pred))\n",
        "    return accuracy_score(original_y, y_pred)\n",
        "\n",
        "#training decision tree model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def decision_tree(random_sample_train):\n",
        "    clf=DecisionTreeClassifier()\n",
        "    clf.fit(random_sample_train.iloc[:,:30],random_sample_train.iloc[:,-1])\n",
        "    y_pred=clf.predict(test_set)\n",
        "    print('accuracy score for decision tree ', accuracy_score(original_y,y_pred))\n",
        "    return accuracy_score(original_y,y_pred)\n",
        "\n",
        "\n",
        "print('random sampling:')\n",
        "logistic.append(logistic_regression(random_sample_train))\n",
        "random.append(random_forest(random_sample_train))\n",
        "svm_li.append(svm(random_sample_train))\n",
        "naive_bayes_li.append(naive_bayes(random_sample_train))\n",
        "decision_tree_li.append(decision_tree(random_sample_train))\n",
        "\n",
        "#using structured sampling\n",
        "\n",
        "# Calculate the number of rows in the dataset\n",
        "n = len(train_set)\n",
        "\n",
        "# Set the sampling interval \"k\" as the square root of the number of rows in the dataset\n",
        "k = 3\n",
        "\n",
        "# Select every \"k\" row starting from a random index in the dataset\n",
        "structured_sample = train_set.iloc[::k]\n",
        "print('structured sampling:',structured_sample.shape)\n",
        "\n",
        "logistic.append(logistic_regression(structured_sample))\n",
        "random.append(random_forest(structured_sample))\n",
        "svm_li.append(svm(structured_sample))\n",
        "naive_bayes_li.append(naive_bayes(structured_sample))\n",
        "decision_tree_li.append(decision_tree(structured_sample))\n",
        "\n",
        "#using stratified sampling\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "X=train_set.iloc[:,:30]\n",
        "Y=train_set.iloc[:,-1]\n",
        "\n",
        "print(train_set.shape)\n",
        "\n",
        "for train_index, test_index in split.split(X,Y):\n",
        "\n",
        " strat_train_set = train_set.iloc[train_index,:]\n",
        " strat_test_set = train_set.iloc[test_index,:]\n",
        "\n",
        "print('data for stratified sampling:')\n",
        "logistic.append(logistic_regression(strat_train_set))\n",
        "random.append(random_forest(strat_train_set))\n",
        "svm_li.append(svm(strat_train_set))\n",
        "naive_bayes_li.append(naive_bayes(strat_train_set))\n",
        "decision_tree_li.append(decision_tree(strat_train_set))\n",
        "\n",
        "#using cluster sampling\n",
        "def get_clustered_Sample(df, n_per_cluster, num_select_clusters):\n",
        "    N = len(df)\n",
        "    K = int(N/n_per_cluster)\n",
        "    data = None\n",
        "    for k in range(K):\n",
        "        sample_k = df.sample(n_per_cluster)\n",
        "        sample_k[\"cluster\"] = np.repeat(k,len(sample_k))\n",
        "        df = df.drop(index = sample_k.index)\n",
        "        data = pd.concat([data,sample_k],axis = 0)\n",
        "\n",
        "    random_chosen_clusters = np.random.randint(0,K,size = num_select_clusters)\n",
        "    samples = data[data.cluster.isin(random_chosen_clusters)]\n",
        "    return(samples)\n",
        "\n",
        "sample = get_clustered_Sample(df = train_set, n_per_cluster = 100, num_select_clusters = 20)\n",
        "sample=sample.iloc[:,:31]\n",
        "print('cluster sampling:')\n",
        "\n",
        "logistic.append(logistic_regression(sample))\n",
        "random.append(random_forest(sample))\n",
        "svm_li.append(svm(sample))\n",
        "naive_bayes_li.append(naive_bayes(sample))\n",
        "decision_tree_li.append(decision_tree(sample))\n",
        "\n",
        "print('best sampling for logistic regression is',dict[logistic.index(max(logistic))])\n",
        "print('best sampling for random forest is',dict[random.index(max(random))])\n",
        "print('best sampling for svm is',dict[svm_li.index(max(svm_li))])\n",
        "print('best sampling for naive bayes is',dict[naive_bayes_li.index(max(naive_bayes_li))])\n",
        "print('best sampling for decision tree is',dict[decision_tree_li.index(max(decision_tree_li))])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "vN9_PxqX64Qi"
      }
    }
  ]
}